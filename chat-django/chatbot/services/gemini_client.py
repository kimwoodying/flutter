# chatbot/services/gemini_client.py
from __future__ import annotations

import hashlib
import logging
import re
import time

import httpx

from chatbot.config import get_settings
from chatbot.models import ChatCache

logger = logging.getLogger(__name__)


# ---------- ê³µí†µ: ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬ ----------
def format_context(text: str) -> str:
    """ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ í¬ë§· ì œê±°."""
    if not text:
        return ""

    cleaned = text
    cleaned = re.sub(r"(ì°¸ê³ ìë£Œ|ì¶œì²˜)", "", cleaned)
    cleaned = re.sub(r"^#{1,6}\s*", "", cleaned, flags=re.MULTILINE)      # ë§ˆí¬ë‹¤ìš´ ì œëª©
    cleaned = re.sub(r"^\s*[-â€¢]\s*", "", cleaned, flags=re.MULTILINE)     # ë¦¬ìŠ¤íŠ¸ ê¸°í˜¸
    cleaned = re.sub(r"\s+", " ", cleaned)
    return cleaned.strip()


def clean_response(text: str) -> str:
    """ìµœì¢… ì‘ë‹µ ë§ˆë¬´ë¦¬ ì •ë¦¬."""
    if not text:
        return ""

    text = text.replace("**", "").replace("__", "").strip()
    # ë„ˆë¬´ ê¸°ë¬˜í•˜ê²Œ ëë‚˜ë©´ ë§ˆì¹¨í‘œ í•˜ë‚˜ ë¶™ì—¬ì£¼ê¸°
    if text and text[-1] not in {".", "!", "?", "~", "ë‹¤"}:
        text += "."
    return text.strip()


# ---------- LLM ì €ìˆ˜ì¤€ í˜¸ì¶œ: Gemini ----------
def _call_gemini(system_prompt: str, user_message: str, temperature: float) -> str:
    settings = get_settings()
    if not settings.gemini_api_key:
        logger.warning("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return ""

    url = (
        "https://generativelanguage.googleapis.com/v1beta/models/"
        "gemini-2.5-flash:generateContent"
    )
    headers = {"Content-Type": "application/json"}
    params = {"key": settings.gemini_api_key}
    body = {
        "contents": [
            {
                "parts": [
                    {"text": system_prompt},
                    {"text": user_message},
                ]
            }
        ],
        "generationConfig": {"temperature": temperature},
    }

    max_attempts = 3
    for attempt in range(max_attempts):
        try:
            with httpx.Client(timeout=40.0) as client:
                resp = client.post(url, params=params, headers=headers, json=body)
                resp.raise_for_status()
            break
        except httpx.HTTPStatusError as exc:
            status = exc.response.status_code
            logger.error("Gemini API error %s: %s", status, exc.response.text)
            if status in {429, 500, 502, 503, 504} and attempt < max_attempts - 1:
                time.sleep(1 + attempt)
                continue
            return ""
        except httpx.RequestError as exc:
            logger.error("Gemini request error: %s", exc)
            if attempt < max_attempts - 1:
                time.sleep(1 + attempt)
                continue
            return ""

    data = resp.json()
    candidates = data.get("candidates") or []
    if not candidates:
        return ""
    parts = candidates[0].get("content", {}).get("parts") or []
    if not parts:
        return ""
    text = parts[0].get("text")
    return text.strip() if isinstance(text, str) else ""


# ---------- LLM ì €ìˆ˜ì¤€ í˜¸ì¶œ: Groq(OpenAI í˜¸í™˜) ----------
def _call_groq(system_prompt: str, user_message: str, temperature: float) -> str:
    settings = get_settings()
    if not getattr(settings, "groq_api_key", None):
        return ""

    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {settings.groq_api_key}",
    }
    body = {
        "model": settings.groq_model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        "temperature": temperature,
    }

    max_attempts = 3
    for attempt in range(max_attempts):
        try:
            with httpx.Client(timeout=40.0) as client:
                resp = client.post(url, headers=headers, json=body)
                resp.raise_for_status()
            break
        except httpx.HTTPStatusError as exc:
            status = exc.response.status_code
            logger.error("Groq API error %s: %s", status, exc.response.text)
            if status in {429, 500, 502, 503, 504} and attempt < max_attempts - 1:
                time.sleep(1 + attempt)
                continue
            return ""
        except httpx.RequestError as exc:
            logger.error("Groq request error: %s", exc)
            if attempt < max_attempts - 1:
                time.sleep(1 + attempt)
                continue
            return ""

    data = resp.json()
    choices = data.get("choices") or []
    if not choices:
        return ""
    content = choices[0].get("message", {}).get("content")
    return content.strip() if isinstance(content, str) else ""


# ---------- LLM Failover ë˜í¼ ----------
def call_llm_with_failover(system_prompt: str, user_message: str, temperature: float) -> str:
    """
    PRIMARY_LLM(env) ê¸°ì¤€ìœ¼ë¡œ ìš°ì„  LLM ì„ íƒ.
    - primary_llm = "gemini" â†’ Gemini ë¨¼ì €, ì‹¤íŒ¨ ì‹œ Groq
    - primary_llm = "groq"   â†’ Groq ë¨¼ì €, ì‹¤íŒ¨ ì‹œ Gemini
    """
    settings = get_settings()
    primary = (getattr(settings, "primary_llm", "gemini") or "gemini").lower()

    def use_gemini() -> str:
        return _call_gemini(system_prompt, user_message, temperature)

    def use_groq() -> str:
        return _call_groq(system_prompt, user_message, temperature)

    if primary == "groq":
        first, second = use_groq, use_gemini
    else:
        first, second = use_gemini, use_groq

    result = first()
    if result:
        return result

    logger.warning("1ì°¨ LLM ì‹¤íŒ¨, ë°±ì—… LLMìœ¼ë¡œ ì‹œë„í•©ë‹ˆë‹¤. primary=%s", primary)
    result = second()
    return result or ""


# ---------- DB ìºì‹œ ìœ í‹¸ ----------
def _make_cache_key(query: str, context: str) -> str:
    raw = (query.strip() + "||" + context.strip()).encode("utf-8")
    return hashlib.sha256(raw).hexdigest()


def _get_cached_response(query: str, context: str) -> str | None:
    key = _make_cache_key(query, context)
    cache = ChatCache.objects.filter(query_hash=key).first()
    if cache:
        cache.hit_count += 1
        cache.save(update_fields=["hit_count"])
        logger.info("ğŸ’¾ DB ìºì‹œ HIT (%s)", cache.query_hash)
        return cache.response
    return None


def _save_cache_response(query: str, context: str, response: str) -> None:
    key = _make_cache_key(query, context)
    try:
        ChatCache.objects.create(
            query_hash=key,
            query=query,
            context=context,
            response=response,
        )
        logger.info("ğŸ’¾ DB ìºì‹œ ì €ì¥ (%s)", key)
    except Exception as exc:
        logger.error("ìºì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: %s", exc)


# ---------- RAG + íŒ¨í„´ë³„ ë§íˆ¬ + DB ìºì‹± ----------
def call_gemini_with_rag(query: str, retrieved_docs: list) -> str:
    """
    RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„± + ì§ˆë¬¸ íŒ¨í„´ë³„ í†¤ ì¡°ì ˆ + DB ê¸°ë°˜ ìºì‹±.
    """

    # 1) ì»¨í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
    parts: list[str] = []
    for d in retrieved_docs:
        text = None
        if isinstance(d, dict):
            text = d.get("text") or d.get("snippet") or d.get("page_content")
        elif isinstance(d, str):
            text = d
        else:
            text = (
                getattr(d, "text", None)
                or getattr(d, "snippet", None)
                or getattr(d, "page_content", None)
            )
        if isinstance(text, str) and text.strip():
            parts.append(text.strip())

    context_raw = " ".join(parts)
    context = format_context(context_raw)

    # 2) DB ìºì‹œ ì¡°íšŒ
    cached = _get_cached_response(query, context)
    if cached:
        return clean_response(cached)

    # 3) ì§ˆë¬¸ íŒ¨í„´ ë¶„ë¥˜
    SYMPTOM_KEYWORDS = [
        "ì•„íŒŒ", "í†µì¦", "ë¶“", "ë¶€ì—ˆ", "ì—´", "ë‘í†µ", "ë³µí†µ",
        "ê°€ìŠ´ì´", "ìˆ¨ì´", "í˜¸í¡", "ê¸°ì¹¨", "ê°€ë˜", "ì–´ì§€ëŸ½", "ì“°ëŸ¬ì§ˆ"
    ]
    EMOTION_KEYWORDS = [
        "ìš°ìš¸", "ë¶ˆì•ˆ", "í˜ë“¤", "ìƒì‹¤ê°", "ì§€ì¹˜", "ë¶ˆí¸í•œ ë§ˆìŒ", "ë©˜íƒˆ",
        "ì£½ê³ ì‹¶", "ì‚´ê¸° ì‹«", "í¬ê¸°í•˜ê³  ì‹¶"
    ]
    TIME_KEYWORDS = [
        "ì‹œê°„", "ëª‡ ì‹œ", "ëª‡ì‹œ", "ìš´ì˜", "ì˜¤í”ˆ", "ë§ˆê°",
        "ì§„ë£Œì‹œê°„", "ì§„ë£Œ ì‹œê°„", "ì–¸ì œê¹Œì§€", "ëª‡ê¹Œì§€ í•´ìš”"
    ]

    def detect_mode(text: str) -> str:
        if any(k in text for k in EMOTION_KEYWORDS):
            return "emotional"
        if any(k in text for k in SYMPTOM_KEYWORDS):
            return "symptom"
        if any(k in text for k in TIME_KEYWORDS):
            return "time"
        return "info"

    mode = detect_mode(query)

    # 4) ê³µí†µ ìŠ¤íƒ€ì¼ ê·œì¹™
    base_style = """
ë‹¹ì‹ ì€ ë³‘ì› ê³µì‹ ì•ˆë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.
ëª¨ë“  ë‹µë³€ì€ ì¡´ëŒ“ë§ë¡œ, 2~4ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•©ë‹ˆë‹¤.
ë¬¸ì¥ì€ ì§§ê³  ëª…í™•í•˜ê²Œ ìœ ì§€í•˜ê³ , ê³¼ë„í•œ ê°íƒ„ì‚¬ë‚˜ ë°˜ë³µ í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
"""

    if mode == "time":
        extra_rule = """
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ì§„ë£Œì‹œê°„ ë˜ëŠ” ìš´ì˜ ì‹œê°„ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤.
ë‹¤ìŒ ë¬¸ì¥ì„ ë°˜ë“œì‹œ í•œ ë²ˆ í¬í•¨í•©ë‹ˆë‹¤:
"ë³‘ì› ì§„ë£Œì‹œê°„ì€ ì˜¤ì „ 9ì‹œë¶€í„° ì˜¤í›„ 5ì‹œ 30ë¶„ê¹Œì§€ì…ë‹ˆë‹¤."
ì´ì™¸ì˜ ì•ˆë‚´ëŠ” ì§ˆë¬¸ ë²”ìœ„ ì•ˆì—ì„œë§Œ ê°„ë‹¨íˆ ì¶”ê°€í•©ë‹ˆë‹¤.
"""
    elif mode == "symptom":
        extra_rule = """
ì‚¬ìš©ìì˜ ì¦ìƒì— ëŒ€í•´ ì¼ë°˜ì ì¸ ì„¤ëª…ê³¼, ì–´ëŠ ì§„ë£Œê³¼ì—ì„œ ìƒë‹´ì„ ë°›ì„ ìˆ˜ ìˆëŠ”ì§€ ì¤‘ì‹¬ìœ¼ë¡œ ì•ˆë‚´í•©ë‹ˆë‹¤.
ì‘ê¸‰ì´ ì˜ì‹¬ë˜ëŠ” ê²½ìš°ì—ë§Œ ë‹¤ìŒ ë¬¸ì¥ì„ ë§ˆì§€ë§‰ì— í•œ ë²ˆ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
"ì¦ìƒì´ ê°‘ìê¸° ì‹¬í•´ì§€ê±°ë‚˜ í˜¸í¡ì´ ê³¤ë€í•´ì§€ëŠ” ê²½ìš° ì‘ê¸‰ì‹¤ ë°©ë¬¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
ì§„ë£Œì‹œê°„ì´ë‚˜ ë¶ˆí•„ìš”í•œ ì¶”ê°€ ì •ë³´ëŠ” ì–¸ê¸‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
"""
    elif mode == "emotional":
        extra_rule = """
ê°ì •ì´ë‚˜ ì‹¬ë¦¬ì  ì–´ë ¤ì›€ì— ëŒ€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ê°ì •ì„ ê°„ë‹¨íˆ ì¸ì •í•˜ë˜, 1~2ë¬¸ì¥ ì´ë‚´ì—ì„œ ì¡°ìš©í•œ í†¤ìœ¼ë¡œ ê³µê° í‘œí˜„ì„ í•˜ê³ ,
í•„ìš” ì‹œ ì „ë¬¸ ìƒë‹´ì´ë‚˜ ì§„ë£Œë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆë‹¤ëŠ” ì •ë„ë¡œ ì•ˆë‚´í•©ë‹ˆë‹¤.
ê³¼ë„í•œ ìœ„ë¡œë‚˜ ì‚¬ì ì¸ ì¡°ì–¸ì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
"""
    else:  # info
        extra_rule = """
ë³‘ì› ì´ìš© ì•ˆë‚´, ì˜ˆì•½ ë°©ë²•, ìœ„ì¹˜, ì¼ë°˜ ì •ë³´ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì…ë‹ˆë‹¤.
ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ì ì¸ ì •ë³´ë§Œ ì„ íƒí•˜ì—¬ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì„œ ë‹µë³€í•©ë‹ˆë‹¤.
ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ ì—†ëŠ” ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
"""

    system_prompt = base_style + extra_rule

    user_message = f"""
[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

[ì°¸ê³ ìš© ì»¨í…ìŠ¤íŠ¸]
ë‹¤ìŒ ë‚´ìš©ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•œ ì°¸ê³  ìë£Œì…ë‹ˆë‹¤.
ì´ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ê³ , ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ ë‹µë³€í•˜ì‹­ì‹œì˜¤.

--- context start ---
{context}
--- context end ---
"""

    # 5) LLM í˜¸ì¶œ (Gemini / Groq Failover)
    raw_reply = call_llm_with_failover(system_prompt, user_message, temperature=0.2)
    if not raw_reply:
        return "í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤."

    final_reply = clean_response(raw_reply)

    # 6) DB ìºì‹œ ì €ì¥
    _save_cache_response(query, context, final_reply)

    return final_reply
