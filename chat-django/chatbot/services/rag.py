from __future__ import annotations

import numpy as np
import hashlib

from chatbot.config import get_settings
from chatbot.services.embeddings import embed_texts
from chatbot.services.vector_store import get_vector_store
from chatbot.services.gemini_client import call_gemini_with_rag
from chatbot.models import ChatCache  # ğŸ”¸ ìºì‹œ ëª¨ë¸ ì¶”ê°€ import


def clean_response(text: str) -> str:
    """
    LLM ì‘ë‹µì„ í”„ë¡ íŠ¸ë¡œ ë³´ë‚´ê¸° ì „ ë§ˆì§€ë§‰ ì •ë¦¬.
    - ë§ˆí¬ë‹¤ìš´ êµµê²Œ(**), ë°‘ì¤„(__) ì œê±°
    - ë¬¸ì¥ ëì´ ì–´ìƒ‰í•˜ë©´ ë§ˆì¹¨í‘œ ì¶”ê°€
    """
    if not text:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

    text = text.replace("**", "").replace("__", "").strip()

    if text and text[-1] not in {".", "!", "?", "ë‹¤", "~"}:
        text += "."

    return text


def run_rag(user_message: str) -> dict:
    """
    ë³‘ì› ì•ˆë‚´ìš© RAG íŒŒì´í”„ë¼ì¸ ì§„ì…ì .

    1) ì‚¬ìš©ì ì§ˆë¬¸ ì„ë² ë”©
    2) FAISS ë²¡í„° ê²€ìƒ‰
    3) ìƒìœ„ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ LLMìœ¼ë¡œ ì „ë‹¬
    4) Gemini/Groq ê¸°ë°˜ í†¤ ì œì–´ëœ ë‹µë³€ ìƒì„±
    """
    try:
        settings = get_settings()

        # 1) ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
        embeddings = embed_texts([user_message])
        if not embeddings:
            raise ValueError("ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        query_vector = np.array(embeddings[0], dtype="float32")

        # 2) FAISS ê²€ìƒ‰
        store = get_vector_store()
        top_k = getattr(settings, "top_k", 5)
        search_results = store.search(query_vector, top_k)

        if not search_results:
            return {
                "reply": (
                    "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•´ ì •í™•í•œ ì•ˆë‚´ê°€ ì–´ë µìŠµë‹ˆë‹¤. "
                    "ìì„¸í•œ ì‚¬í•­ì€ ë³‘ì› ëŒ€í‘œë²ˆí˜¸(042-000-0000)ë¡œ ë¬¸ì˜í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤."
                ),
                "sources": [],
            }

        # (score, metadata) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ â†’ ìœ ì‚¬ë„ threshold ì ìš©
        min_score = 0.5
        relevant_results = [(score, meta) for score, meta in search_results if score > min_score]

        # threshold ë„˜ëŠ” ê²Œ ì—†ìœ¼ë©´ ìµœìƒìœ„ í•˜ë‚˜ë§Œì´ë¼ë„ ì‚¬ìš©
        if not relevant_results:
            relevant_results = [search_results[0]]

        # ìƒìœ„ ëª‡ ê°œë§Œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
        max_docs = 3
        contexts = [meta for _, meta in relevant_results[:max_docs]]

        if not contexts:
            return {
                "reply": (
                    "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•´ ì •í™•í•œ ì•ˆë‚´ê°€ ì–´ë µìŠµë‹ˆë‹¤. "
                    "ìì„¸í•œ ì‚¬í•­ì€ ë³‘ì› ëŒ€í‘œë²ˆí˜¸(042-000-0000)ë¡œ ë¬¸ì˜í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤."
                ),
                "sources": [],
            }

        # 3) LLM í˜¸ì¶œ
        raw_reply = call_gemini_with_rag(user_message, contexts)
        if not raw_reply:
            return {
                "reply": (
                    "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. "
                    "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œê±°ë‚˜, ë³‘ì› ëŒ€í‘œë²ˆí˜¸(042-000-0000)ë¡œ ë¬¸ì˜í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤."
                ),
                "sources": [],
            }

        reply_text = clean_response(raw_reply)

        # 4) ì¶œì²˜ ì •ë³´ â†’ ì§€ê¸ˆì€ ìˆ¨ê¸°ê³  ë¹ˆ ë¦¬ìŠ¤íŠ¸ë§Œ
        return {
            "reply": reply_text,
            "sources": [],
        }

    except Exception as e:
        import traceback

        print(f"Error in RAG pipeline: {str(e)}\n{traceback.format_exc()}")
        return {
            "reply": (
                "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. "
                "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤."
            ),
            "sources": [],
        }


# =========================
# ğŸ”¥ ì—¬ê¸°ë¶€í„° ìºì‹± ë˜í¼ ì¶”ê°€
# =========================

def _make_query_hash(user_message: str) -> str:
    """
    ì§ˆë¬¸ ë¬¸ìì—´ë§Œ ê¸°ì¤€ìœ¼ë¡œ SHA-256 í•´ì‹œ ìƒì„±.
    (ë‚˜ì¤‘ì— ì»¨í…ìŠ¤íŠ¸ ë²„ì „ê¹Œì§€ ë„£ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸°ì„œ ì„ì–´ì¤˜ë„ ë¨)
    """
    base = user_message.strip().lower().encode("utf-8")
    return hashlib.sha256(base).hexdigest()


def run_rag_with_cache(user_message: str) -> dict:
    """
    1) DB(ChatCache)ì—ì„œ ë™ì¼ ì§ˆë¬¸ ìºì‹œ ì¡°íšŒ
    2) ìˆìœ¼ë©´ â†’ ë°”ë¡œ ë¦¬í„´ (hit_count ì¦ê°€)
    3) ì—†ìœ¼ë©´ â†’ ê¸°ì¡´ run_rag ì‹¤í–‰ í›„ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
    """
    query = user_message.strip()
    if not query:
        return {
            "reply": "ì§ˆë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”.",
            "sources": [],
        }

    qh = _make_query_hash(query)

    # 1) ìºì‹œ ì¡°íšŒ
    try:
        cached = ChatCache.objects.filter(query_hash=qh).first()
    except Exception as e:
        print(f"[ChatCache] ì¡°íšŒ ì˜¤ë¥˜: {e}")
        cached = None

    if cached:
        try:
            cached.hit_count += 1
            cached.save(update_fields=["hit_count"])
        except Exception as e:
            print(f"[ChatCache] hit_count ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")

        return {
            "reply": cached.response,
            "sources": [],   # ìºì‹œì—ì„œë„ sourcesëŠ” ë¹„ì›Œì„œ ë¦¬í„´
        }

    # 2) ìºì‹œ ì—†ìœ¼ë©´ â†’ ì›ë˜ RAG ì‹¤í–‰
    result = run_rag(query)
    reply_text = result.get("reply") or ""

    # 3) ìºì‹œ ì €ì¥
    if reply_text:
        try:
            ChatCache.objects.create(
                query_hash=qh,
                query=query,
                context="",     # ë‚˜ì¤‘ì— ì»¨í…ìŠ¤íŠ¸ ì „ë¬¸ê¹Œì§€ ì €ì¥í•˜ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸° ì±„ì›Œë„ ë¨
                response=reply_text,
                hit_count=1,
            )
        except Exception as e:
            print(f"[ChatCache] ì €ì¥ ì˜¤ë¥˜: {e}")

    return result
